{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMrs3zGVQdfU"
   },
   "source": [
    "# Assignment 17: Kiểm tra đạo văn\n",
    "\n",
    "Tổng quan: Ở bài tập này chúng ta sẽ lần lượt thực hành các bước để xây dựng một ứng dụng kiểm tra đạo văn sử dụng một mô hình pre-trained word2vec. \n",
    "\n",
    "- Dữ liệu được lưu trong hai file: en_source_data.txt chứa văn bản cần kiểm tra, en_target_data.txt chứa các văn bản đối chiếu.\n",
    "- Bài tập yêu cầu các kiến thức về lập trình Python với các thư viện: gensim (để load word2vec model), numpy (tính similarity). Ngoài ra chúng ta sử dụng một pre-trained word2vec model (sử dụng [Google's pre-trained word2vec model](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp311-cp311-win_amd64.whl.metadata (8.2 kB)\n",
      "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl.metadata (61 kB)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Downloading scipy-1.13.1-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\dnhtr\\miniconda3\\envs\\myenv\\lib\\site-packages (from gensim) (7.3.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\dnhtr\\miniconda3\\envs\\myenv\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
      "Downloading gensim-4.3.3-cp311-cp311-win_amd64.whl (24.0 MB)\n",
      "   ---------------------------------------- 0.0/24.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.0/24.0 MB 8.5 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 3.4/24.0 MB 10.6 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 7.1/24.0 MB 12.8 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 10.5/24.0 MB 13.9 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 15.7/24.0 MB 16.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 20.7/24.0 MB 18.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.0/24.0 MB 17.1 MB/s eta 0:00:00\n",
      "Downloading numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "   ---------------------------------------- 0.0/15.8 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 5.0/15.8 MB 23.2 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 9.4/15.8 MB 23.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 14.2/15.8 MB 23.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.8/15.8 MB 21.6 MB/s eta 0:00:00\n",
      "Downloading scipy-1.13.1-cp311-cp311-win_amd64.whl (46.2 MB)\n",
      "   ---------------------------------------- 0.0/46.2 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 4.7/46.2 MB 22.0 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 9.7/46.2 MB 22.4 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 13.9/46.2 MB 21.8 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 18.4/46.2 MB 21.8 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 23.1/46.2 MB 21.8 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 26.7/46.2 MB 20.9 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 31.5/46.2 MB 21.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 35.9/46.2 MB 21.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 40.1/46.2 MB 21.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 42.7/46.2 MB 20.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  46.1/46.2 MB 19.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 46.2/46.2 MB 18.6 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy, scipy, gensim\n",
      "\n",
      "  Attempting uninstall: numpy\n",
      "\n",
      "    Found existing installation: numpy 2.0.1\n",
      "\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "    Uninstalling numpy-2.0.1:\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "      Successfully uninstalled numpy-2.0.1\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "  Attempting uninstall: scipy\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "    Found existing installation: scipy 1.16.1\n",
      "   ---------------------------------------- 0/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "    Uninstalling scipy-1.16.1:\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "      Successfully uninstalled scipy-1.16.1\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   ---------------------------------------- 3/3 [gensim]\n",
      "\n",
      "Successfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'GoogleNews-vectors-negative300.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Path to the downloaded GoogleNews-vectors-negative300.bin file\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# If it's a .gz file, ensure it's unzipped or adjust the filename accordingly.\u001b[39;00m\n\u001b[32m      5\u001b[39m model_path = \u001b[33m'\u001b[39m\u001b[33mGoogleNews-vectors-negative300.bin\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m model = \u001b[43mKeyedVectors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mModel loaded successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# You can now use the model, for example:\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dnhtr\\miniconda3\\envs\\myenv\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:1719\u001b[39m, in \u001b[36mKeyedVectors.load_word2vec_format\u001b[39m\u001b[34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[39m\n\u001b[32m   1672\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m   1673\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_word2vec_format\u001b[39m(\n\u001b[32m   1674\u001b[39m         \u001b[38;5;28mcls\u001b[39m, fname, fvocab=\u001b[38;5;28;01mNone\u001b[39;00m, binary=\u001b[38;5;28;01mFalse\u001b[39;00m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf8\u001b[39m\u001b[33m'\u001b[39m, unicode_errors=\u001b[33m'\u001b[39m\u001b[33mstrict\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   1675\u001b[39m         limit=\u001b[38;5;28;01mNone\u001b[39;00m, datatype=REAL, no_header=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1676\u001b[39m     ):\n\u001b[32m   1677\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[32m   1678\u001b[39m \n\u001b[32m   1679\u001b[39m \u001b[33;03m    Warnings\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1717\u001b[39m \n\u001b[32m   1718\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1720\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfvocab\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbinary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43municode_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[43municode_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1721\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatatype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdatatype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_header\u001b[49m\u001b[43m=\u001b[49m\u001b[43mno_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1722\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dnhtr\\miniconda3\\envs\\myenv\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:2048\u001b[39m, in \u001b[36m_load_word2vec_format\u001b[39m\u001b[34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[39m\n\u001b[32m   2045\u001b[39m             counts[word] = \u001b[38;5;28mint\u001b[39m(count)\n\u001b[32m   2047\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mloading projection weights from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, fname)\n\u001b[32m-> \u001b[39m\u001b[32m2048\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[32m   2049\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m no_header:\n\u001b[32m   2050\u001b[39m         \u001b[38;5;66;03m# deduce both vocab_size & vector_size from 1st pass over file\u001b[39;00m\n\u001b[32m   2051\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m binary:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dnhtr\\miniconda3\\envs\\myenv\\Lib\\site-packages\\smart_open\\smart_open_lib.py:170\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[39m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m transport_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    168\u001b[39m     transport_params = {}\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m fobj = \u001b[43m_shortcut_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m    \u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fobj\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dnhtr\\miniconda3\\envs\\myenv\\Lib\\site-packages\\smart_open\\smart_open_lib.py:368\u001b[39m, in \u001b[36m_shortcut_open\u001b[39m\u001b[34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[39m\n\u001b[32m    365\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mb\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m    366\u001b[39m     open_kwargs[\u001b[33m'\u001b[39m\u001b[33merrors\u001b[39m\u001b[33m'\u001b[39m] = errors\n\u001b[32m--> \u001b[39m\u001b[32m368\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_builtin_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mopen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'GoogleNews-vectors-negative300.bin'"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Path to the downloaded GoogleNews-vectors-negative300.bin file\n",
    "# If it's a .gz file, ensure it's unzipped or adjust the filename accordingly.\n",
    "model_path = 'GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# You can now use the model, for example:\n",
    "print(model.most_similar('king'))\n",
    "print(model['word'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "25EYQWZnQdfv"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeyedvectors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KeyedVectors\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oCzz0gZVQdf0"
   },
   "source": [
    "## Câu hỏi 1: khảo sát tập dữ liệu\n",
    "Hãy lập trình đoạn chương trình dưới đây để khảo sát về số lượng và quan sát một vài ví dụ về bộ dữ liệu.\n",
    "\n",
    "Gợi ý: \n",
    "- Đọc tài liệu cần kiểm tra, từ file \"./en_source_data.txt\", lưu vào biến source_doc. \n",
    "- Đọc các tài liệu đối chiếu từ file \"./en_target_data.txt\", rồi lưu vào biến target_docs (kiểu list, mỗi doc là một phần tử trong list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MuS76cp2Qdf2",
    "outputId": "a92ea412-ff08-49ca-cd2e-6cb16dfa6d89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_doc:\n",
      " Over the long term, the durability of attitudes toward Trump spotlights the likelihood of a widening rift between two Americas fundamentally diverging in both their exposure to and attitudes about such fundamental dynamics as the nation's growing racial and religious diversity, rising demands for greater racial equality, changing gender roles and the transition from an industrial to an information age economy.\n",
      "\n",
      "target_doc_num: 23\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "\n",
    "\n",
    "\n",
    "#### YOUR CODE HERE ####\n",
    "print(\"source_doc:\\n\",source_doc)\n",
    "print(\"target_doc_num:\", len(target_docs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9sC3LXwQdf8"
   },
   "source": [
    "## Câu hỏi 2: Xây dựng một class cho việc tính document similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YuVRUdssQdf-"
   },
   "outputs": [],
   "source": [
    "class DocSim:\n",
    "    def __init__(self, w2v_model, stopwords=None):\n",
    "        self.w2v_model = w2v_model\n",
    "        self.stopwords = stopwords if stopwords is not None else []\n",
    "\n",
    "    \"\"\"\n",
    "    Câu hỏi 2.1:\n",
    "    Xác định giá trị các vectors cho từng từ trong document. Đầu tiên, tiền\n",
    "    xử lý tài liệu đầu vào. Sau đó, với mỗi từ ta tính word2vec cho từng từ\n",
    "    trong văn bản. Cuối cùng, ta tính trung bình các vectors của các từ\n",
    "    trong văn bản đó. Câu hỏi trong bài tập này là tính giá trị embedding\n",
    "    của từng từ (cú pháp: self.w2v_model[<từ_cần_tính>]), rồi gán vào biến vec\n",
    "    \"\"\"\n",
    "    def vectorize(self, doc: str) -> np.ndarray:\n",
    "        doc = doc.lower()\n",
    "        words = [w for w in doc.split(\" \") if w not in self.stopwords]\n",
    "        word_vecs = []\n",
    "        for word in words:\n",
    "            try:\n",
    "                #### YOUR CODE HERE ####\n",
    "                \n",
    "                \n",
    "                \n",
    "                #### YOUR CODE HERE ####\n",
    "                word_vecs.append(vec)\n",
    "            except KeyError:\n",
    "                # Ignore, if the word doesn't exist in the vocabulary\n",
    "                pass\n",
    "\n",
    "        vector = np.mean(word_vecs, axis=0)\n",
    "        return vector\n",
    "\n",
    "    def _cosine_sim(self, vecA, vecB):\n",
    "        \"\"\"\n",
    "        Câu hỏi 2.2:\n",
    "        Tính cosine similarity giữa hai vectors (vecA, vecB)\n",
    "        \"\"\"\n",
    "        #### YOUR CODE HERE ####\n",
    "        \n",
    "        \n",
    "        \n",
    "        #### YOUR CODE HERE ####\n",
    "        \n",
    "        if np.isnan(np.sum(csim)):\n",
    "            return 0\n",
    "        return csim\n",
    "        \"\"\"\n",
    "        Câu hỏi 2.3:\n",
    "        Hàm dưới dùng để tính giá trị similarity giữa hai tài liệu cần\n",
    "        so sánh. Hoàn thành việc tính độ tương tự giữa hai vectors (source_vec,\n",
    "        target_vec)\n",
    "        Gợi ý: dùng hàm _cosine_sim()\n",
    "        \"\"\"\n",
    "    def calculate_similarity(self, source_doc, target_docs=None, threshold=0.9):\n",
    "        \"\"\"Calculates & returns similarity scores between given source document & all\n",
    "        the target documents.\"\"\"\n",
    "        if not target_docs:\n",
    "            return []\n",
    "\n",
    "        if isinstance(target_docs, str):\n",
    "            target_docs = [target_docs]\n",
    "\n",
    "        source_vec = self.vectorize(source_doc)\n",
    "        results = []\n",
    "        for doc in target_docs:\n",
    "            target_vec = self.vectorize(doc)\n",
    "            #### YOUR CODE HERE ####\n",
    "            \n",
    "           \n",
    "            \n",
    "            #### YOUR CODE HERE ####\n",
    "            if sim_score > threshold:\n",
    "                results.append({\"score\": sim_score, \"doc\": doc})\n",
    "            # Sort results by score in desc order\n",
    "            results.sort(key=lambda k: k[\"score\"], reverse=True)\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZkMI4oWMQdgC"
   },
   "source": [
    "## Câu hỏi 3: Tạo đối tượng\n",
    "Sau khi đã tạo xong lớp để tính toán similarity giữa các docs (DocSim), chúng ta đi tạo đối tượng cho lớp trên:\n",
    "\n",
    "- Load pre-trained word embedding model\n",
    "- Tạo danh sách từ dừng\n",
    "- Sau cùng là tạo đối tượng cho lớp DocSim\n",
    "\n",
    "Gợi ý: tạo đối tượng ds của lớp DocSim() với hai tham số là model và stopwords=stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ukB_qODdQdgD"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KeyedVectors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m googlenews_model_path = \u001b[33m'\u001b[39m\u001b[33mGoogleNews-vectors-negative300.bin\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      2\u001b[39m stopwords_path = \u001b[33m'\u001b[39m\u001b[33m./stopwords_en.txt\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m model = \u001b[43mKeyedVectors\u001b[49m.load_word2vec_format(googlenews_model_path, binary=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(stopwords_path, \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fh:\n\u001b[32m      5\u001b[39m     stopwords = fh.read().split(\u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'KeyedVectors' is not defined"
     ]
    }
   ],
   "source": [
    "googlenews_model_path = 'GoogleNews-vectors-negative300.bin'\n",
    "stopwords_path = './stopwords_en.txt'\n",
    "model = KeyedVectors.load_word2vec_format(googlenews_model_path, binary=True)\n",
    "with open(stopwords_path, 'r') as fh:\n",
    "    stopwords = fh.read().split(\",\")\n",
    "#### YOUR CODE HERE ####\n",
    "\n",
    "ds = DocSim(model, stopwords=stopwords)\n",
    "\n",
    "#### YOUR CODE HERE ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5iZLUfUdQdgF"
   },
   "source": [
    "## Câu hỏi 4: Kiểm tra đạo văn cho một doc mới\n",
    "Gợi ý: Sử dụng hàm ds.calculate_similarity() để tính độ tương tự gữa tài liệu cần kiểm tra source_doc, và các tài liệu đối chiếu (target_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SWf3hdqGQdgG",
    "outputId": "e42e55a7-999e-4fa2-c24b-b1168938558e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 1.0, 'doc': \"Over the long term, the durability of attitudes toward Trump spotlights the likelihood of a widening rift between two Americas fundamentally diverging in both their exposure to and attitudes about such fundamental dynamics as the nation's growing racial and religious diversity, rising demands for greater racial equality, changing gender roles and the transition from an industrial to an information age economy.\\n\"}]\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "\n",
    "\n",
    "\n",
    "#### YOUR CODE HERE ####\n",
    "print(sim_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1IMimvu_QdgH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment17.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
