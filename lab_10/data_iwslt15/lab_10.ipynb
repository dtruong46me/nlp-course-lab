{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2aa321aa",
      "metadata": {
        "id": "2aa321aa"
      },
      "source": [
        "# Assignment 10: Machine Translation with Seq2Seq Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c2b2572",
      "metadata": {
        "id": "8c2b2572"
      },
      "source": [
        "## 1. Configuration & Constants\n",
        "\n",
        "- Định nghĩa các biến cấu hình và hằng số cần thiết cho quá trình huấn luyện mô hình dịch máy."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U tensorflow keras -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "H5a9Qu9Y8wBP",
        "outputId": "a2301e43-2e3f-4627-cf43-7f363a27d75b"
      },
      "id": "H5a9Qu9Y8wBP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Collecting keras\n",
            "  Downloading keras-3.11.3-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google_pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt_einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf>=5.28.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing_extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.75.1)\n",
            "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras) (0.17.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow) (3.9)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow) (11.3.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
            "Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.7/620.7 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-3.11.3-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m118.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboard, keras, tensorflow\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.19.0\n",
            "    Uninstalling tensorboard-2.19.0:\n",
            "      Successfully uninstalled tensorboard-2.19.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.10.0\n",
            "    Uninstalling keras-3.10.0:\n",
            "      Successfully uninstalled keras-3.10.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.19.0\n",
            "    Uninstalling tensorflow-2.19.0:\n",
            "      Successfully uninstalled tensorflow-2.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show tensorflow\n",
        "!pip show keras"
      ],
      "metadata": {
        "id": "isRwaGgt9cMO",
        "outputId": "fc0562d1-3804-49be-cc35-807d3ca7534d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "isRwaGgt9cMO",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: tensorflow\n",
            "Version: 2.20.0\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: absl-py, astunparse, flatbuffers, gast, google_pasta, grpcio, h5py, keras, libclang, ml_dtypes, numpy, opt_einsum, packaging, protobuf, requests, setuptools, six, tensorboard, termcolor, typing_extensions, wrapt\n",
            "Required-by: dopamine_rl, tensorflow-text, tensorflow_decision_forests, tf_keras\n",
            "Name: keras\n",
            "Version: 3.11.3\n",
            "Summary: Multi-backend Keras\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: Keras team <keras-users@googlegroups.com>\n",
            "License: Apache License 2.0\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: absl-py, h5py, ml-dtypes, namex, numpy, optree, packaging, rich\n",
            "Required-by: keras-hub, tensorflow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "f83a9e1c",
      "metadata": {
        "id": "f83a9e1c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "import time\n",
        "from typing import Dict, Tuple, List\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import keras\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "7a674761",
      "metadata": {
        "id": "7a674761"
      },
      "outputs": [],
      "source": [
        "# Data configuration\n",
        "__root__ = os.getcwd()\n",
        "DATA_DIR = \"data_iwslt15\"\n",
        "SITE_PREFIX = \"https://nlp.stanford.edu/projects/nmt/data\"\n",
        "DATA_FILES = {\n",
        "    \"train\": (\"train.en\", \"train.vi\"),\n",
        "    \"dev\": (\"tst2012.en\", \"tst2012.vi\"),\n",
        "    \"test\": (\"tst2013.en\", \"tst2013.vi\"),\n",
        "}\n",
        "NUM_EXAMPLES = 50000 # Number of training examples to use\n",
        "MAX_SENTENCE_LENGTH = 50 # Max number of tokens per sentence\n",
        "\n",
        "# Model Hyperparameters\n",
        "BUFFER_SIZE = 32000 # Buffer size for shuffling the dataset\n",
        "BATCH_SIZE = 64 # Batch size for training\n",
        "EMBEDDING_DIM = 512 # Dimension of the embedding vector\n",
        "HIDDEN_UNITS = 512 # Number of hidden units in the LSTM\n",
        "EPOCHS = 10 # Number of epochs to train the model\n",
        "\n",
        "# Training configuration\n",
        "CHECKPOINT_DIR = os.path.join(__root__, \"lab_10\", \"model_checkpoints\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74c91022",
      "metadata": {
        "id": "74c91022"
      },
      "source": [
        "## 2. Data preparation\n",
        "\n",
        "- Download IWSLT15 dataset từ [Stanford NMT](https://nlp.stanford.edu/projects/nmt/data/).\n",
        "- Giải nén và lưu vào thư mục `/data_iwslt15`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "d136af0b",
      "metadata": {
        "id": "d136af0b"
      },
      "outputs": [],
      "source": [
        "def preprocess_sentence(sentence: str) -> str:\n",
        "    \"\"\"\n",
        "    Add <start> and <end> tokens to the sentence.\n",
        "    \"\"\"\n",
        "    return f\"<start> {sentence.strip()} <end>\"\n",
        "\n",
        "def tokenize_sentences(\n",
        "        sentences: List[str]\n",
        ") -> Tuple[tf.Tensor, tf.keras.preprocessing.text.Tokenizer]: # type: ignore\n",
        "    \"\"\"\n",
        "    Tokenize and pad a list of sentences.\n",
        "    Args:\n",
        "        sentences (List[str]): List of sentences to tokenize.\n",
        "    Returns:\n",
        "        Tuple[tf.Tensor, tf.keras.preprocessing.text.Tokenizer]: A tuple containing the padded tensor and fitted tokenizer\n",
        "    \"\"\"\n",
        "    # Create a tokenizer and fit on the sentences\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='') # type: ignore\n",
        "\n",
        "    # Fit the tokenizer on the sentences\n",
        "    tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "    # Convert sentences to sequences and pad them\n",
        "    tensor = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "    # Pad the sequences to ensure uniform length\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post') # type: ignore\n",
        "\n",
        "    return tensor, tokenizer\n",
        "\n",
        "def load_data(\n",
        "        source_path: str,\n",
        "        target_path: str,\n",
        "        num_examples: int = None,\n",
        ") -> Tuple[List[str], List[str]]:\n",
        "    \"\"\"\n",
        "    Load and preprocess sentence pairs from file paths.\n",
        "    Args:\n",
        "        source_path (str): Path to the source language file.\n",
        "        target_path (str): Path to the target language file.\n",
        "        num_examples (int, optional): Number of examples to load. If None, load all.\n",
        "    Returns:\n",
        "        Tuple[List[str], List[str]]: Lists of preprocessed source and target sentences.\n",
        "    \"\"\"\n",
        "    # Read source sentences\n",
        "    with open(source_path, 'r', encoding='utf-8') as f:\n",
        "        source_sentences = f.readlines()\n",
        "\n",
        "    # Read target sentences\n",
        "    with open(target_path, 'r', encoding='utf-8') as f:\n",
        "        target_sentences = f.readlines()\n",
        "\n",
        "    # Assuming both files have the same number of lines\n",
        "    assert len(source_sentences) == len(target_sentences)\n",
        "\n",
        "    # Get the number of examples to use\n",
        "    if num_examples:\n",
        "        source_sentences = source_sentences[:num_examples]\n",
        "        target_sentences = target_sentences[:num_examples]\n",
        "\n",
        "    # Preprocess sentences\n",
        "    source_data, target_data = [], []\n",
        "    for src, tgt in zip(source_sentences, target_sentences):\n",
        "        if len(src.split()) <= MAX_SENTENCE_LENGTH and len(tgt.split()) <= MAX_SENTENCE_LENGTH:\n",
        "            source_data.append(preprocess_sentence(src))\n",
        "            target_data.append(preprocess_sentence(tgt))\n",
        "\n",
        "    return source_data, target_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "a6ffe48d",
      "metadata": {
        "id": "a6ffe48d"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess the data\n",
        "DATA_PARENT_DIR = os.path.join(__root__, DATA_DIR)\n",
        "\n",
        "TRAIN_SOURCE_PATH = os.path.join(DATA_PARENT_DIR, DATA_FILES['train'][0])\n",
        "TRAIN_TARGET_PATH = os.path.join(DATA_PARENT_DIR, DATA_FILES['train'][1])\n",
        "source_sentences, target_sentences = load_data(\n",
        "    TRAIN_SOURCE_PATH,\n",
        "    TRAIN_TARGET_PATH,\n",
        "    num_examples=NUM_EXAMPLES\n",
        ")\n",
        "\n",
        "# Tokenize the sentences\n",
        "source_tensor, source_tokenizer = tokenize_sentences(source_sentences)\n",
        "target_tensor, target_tokenizer = tokenize_sentences(target_sentences)\n",
        "\n",
        "# Create training and validation sets\n",
        "vocab_src_size = len(source_tokenizer.word_index) + 1 # Why +1? Because of padding token\n",
        "vocab_tgt_size = len(target_tokenizer.word_index) + 1\n",
        "\n",
        "# Create a tf.data dataset from the tensors and batch it\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(tensors=(source_tensor, target_tensor))\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "828566bd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "828566bd",
        "outputId": "3fde77a0-796d-4b7a-c084-c122de027a11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input batch shape: (64, 52), Target batch shape: (64, 52)\n"
          ]
        }
      ],
      "source": [
        "example_input_batch, example_target_batch = next(iter(train_dataset)) # type: ignore\n",
        "print(f\"Input batch shape: {example_input_batch.shape}, Target batch shape: {example_target_batch.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dfb1651",
      "metadata": {
        "id": "0dfb1651"
      },
      "source": [
        "## 3. Model Architecture (Seq2Seq with Attention)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60973698",
      "metadata": {
        "id": "60973698"
      },
      "source": [
        "### 3.1. Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "e4a355a5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4a355a5",
        "outputId": "87565d6b-da8f-41f3-c872-92b797c25432"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder output shape: (64, 52, 512)\n",
            "Encoder hidden state shape: (64, 512)\n"
          ]
        }
      ],
      "source": [
        "class Encoder(tf.keras.Model): # type: ignore\n",
        "    def __init__(self, vocab_size: int, embedding_dim: int, hidden_units: int, batch_size: int):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.hidden_units = hidden_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) # type: ignore\n",
        "        self.gru = tf.keras.layers.GRU(\n",
        "            self.hidden_units,\n",
        "            return_sequences=True,\n",
        "            return_state=True,\n",
        "            recurrent_initializer='glorot_uniform'\n",
        "        )\n",
        "\n",
        "    def call(self, x: tf.Tensor, hidden: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "        x = self.embedding(x)\n",
        "        output_tuple = self.gru(x, initial_state=hidden)\n",
        "        output = output_tuple[0]\n",
        "        state = tf.stack(output_tuple[1:], axis=0)\n",
        "        return output, state\n",
        "\n",
        "    def initialize_hidden_state(self) -> tf.Tensor:\n",
        "        return tf.zeros((self.batch_size, self.hidden_units))\n",
        "\n",
        "encoder = Encoder(vocab_size=vocab_src_size, embedding_dim=EMBEDDING_DIM, hidden_units=HIDDEN_UNITS, batch_size=BATCH_SIZE)\n",
        "sample_output, sample_hidden = encoder(example_input_batch, encoder.initialize_hidden_state())\n",
        "print(f\"Encoder output shape: {sample_output.shape}\") # (batch_size, max_length, hidden_units)\n",
        "print(f\"Encoder hidden state shape: {sample_hidden.shape}\") # (batch_size, hidden_units)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a301206",
      "metadata": {
        "id": "2a301206"
      },
      "source": [
        "### 3.2. Bahdanau Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "98df4a10",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98df4a10",
        "outputId": "6bc74a21-8c80-4e22-f5e2-895738788d60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention result shape: (64, 512)\n",
            "Attention weights shape: (64, 52, 1)\n"
          ]
        }
      ],
      "source": [
        "class BahdanauAttention(tf.keras.Model): # type: ignore\n",
        "    def __init__(self, units: int):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units) # type: ignore\n",
        "        self.W2 = tf.keras.layers.Dense(units) # type: ignore\n",
        "        self.V = tf.keras.layers.Dense(1) # type: ignore\n",
        "\n",
        "    def call(self, query: tf.Tensor, values: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "        # query shape == (batch_size, hidden size)\n",
        "        # values shape == (batch_size, max_len, hidden size)\n",
        "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "        query_with_time_axis = tf.expand_dims(query, 1) # (batch_size, 1, hidden size)\n",
        "\n",
        "        # score shape == (batch_size, max_length, 1)\n",
        "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(query_with_time_axis)))\n",
        "\n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * values # type: ignore\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights # type: ignore\n",
        "\n",
        "attention_layer = BahdanauAttention(units=HIDDEN_UNITS)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "print(f\"Attention result shape: {attention_result.shape}\") # (batch_size, hidden_units)\n",
        "print(f\"Attention weights shape: {attention_weights.shape}\") # (batch_size, max_length, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a35d890d",
      "metadata": {
        "id": "a35d890d"
      },
      "source": [
        "### 3.3. Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "85d9dbc7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85d9dbc7",
        "outputId": "aca264ac-4357-4b56-a80d-7d8042932621"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder output shape: (64, 11252)\n"
          ]
        }
      ],
      "source": [
        "class Decoder(tf.keras.Model): # type: ignore\n",
        "    def __init__(self, vocab_size: int, embedding_dim: int, hidden_units: int, batch_size: int):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.hidden_units = hidden_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) # type: ignore\n",
        "        self.gru = tf.keras.layers.GRU( # type: ignore\n",
        "            self.hidden_units,\n",
        "            return_sequences=True,\n",
        "            return_state=True,\n",
        "            recurrent_initializer='glorot_uniform'\n",
        "        )\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size) # type: ignore\n",
        "\n",
        "        # Used for attention\n",
        "        self.attention = BahdanauAttention(self.hidden_units)\n",
        "\n",
        "    def call(self, x, hidden, enc_output):\n",
        "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "        # x shape after embedding == (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # x shape after concat == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "        # passing the concatenated vector to the GRU\n",
        "        output_tuple = self.gru(x, initial_state=hidden)\n",
        "        output = output_tuple[0]\n",
        "        state = tf.stack(output_tuple[1:], axis=0)\n",
        "\n",
        "        # output shape == (batch_size * 1, hidden_size)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "        # output shape == (batch_size, vocab_size)\n",
        "        x = self.fc(output)\n",
        "\n",
        "        return x, state, attention_weights\n",
        "\n",
        "decoder = Decoder(vocab_size=vocab_tgt_size, embedding_dim=EMBEDDING_DIM, hidden_units=HIDDEN_UNITS, batch_size=BATCH_SIZE)\n",
        "sample_decoder_output, _, _ = decoder(\n",
        "    tf.random.uniform((BATCH_SIZE, 1)),\n",
        "    sample_hidden,\n",
        "    sample_output\n",
        ")\n",
        "print(f\"Decoder output shape: {sample_decoder_output.shape}\") # (batch_size, vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db7193a0",
      "metadata": {
        "id": "db7193a0"
      },
      "source": [
        "### 3.4. Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "d31e0549",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        },
        "id": "d31e0549",
        "outputId": "605a99fe-eafe-427d-e166-aee66c992884"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OperatorNotAllowedInGraphError",
          "evalue": "in user code:\n\n    File \"/tmp/ipython-input-559456271.py\", line 41, in train_step  *\n        enc_output, enc_hidden = encoder(source, enc_hidden)\n    File \"/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/ipython-input-1804359599.py\", line 16, in call\n        output_tuple = self.gru(x, initial_state=hidden)\n\n    OperatorNotAllowedInGraphError: Exception encountered when calling GRU.call().\n    \n    \u001b[1mIterating over a symbolic `tf.Tensor` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\u001b[0m\n    \n    Arguments received by GRU.call():\n      • sequences=tf.Tensor(shape=(64, 52, 512), dtype=float32)\n      • initial_state=tf.Tensor(shape=(64, 512), dtype=float32)\n      • mask=None\n      • training=False\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-559456271.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;31m# Start training the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-559456271.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTEP_PER_EPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/autograph_util.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m: in user code:\n\n    File \"/tmp/ipython-input-559456271.py\", line 41, in train_step  *\n        enc_output, enc_hidden = encoder(source, enc_hidden)\n    File \"/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/ipython-input-1804359599.py\", line 16, in call\n        output_tuple = self.gru(x, initial_state=hidden)\n\n    OperatorNotAllowedInGraphError: Exception encountered when calling GRU.call().\n    \n    \u001b[1mIterating over a symbolic `tf.Tensor` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\u001b[0m\n    \n    Arguments received by GRU.call():\n      • sequences=tf.Tensor(shape=(64, 52, 512), dtype=float32)\n      • initial_state=tf.Tensor(shape=(64, 512), dtype=float32)\n      • mask=None\n      • training=False\n"
          ]
        }
      ],
      "source": [
        "optimizer = tf.keras.optimizers.Adam() # type: ignore\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none') # type: ignore\n",
        "\n",
        "def loss_function(real: tf.Tensor, pred: tf.Tensor) -> tf.Tensor:\n",
        "    \"\"\"\n",
        "    Compute the loss between the real and predicted values.\n",
        "    Args:\n",
        "        real (tf.Tensor): The ground truth tensor.\n",
        "        pred (tf.Tensor): The predicted tensor from the model.\n",
        "    Returns:\n",
        "        tf.Tensor: The computed loss value.\n",
        "    \"\"\"\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "@tf.function\n",
        "def train_step(\n",
        "    source: tf.Tensor,\n",
        "    target: tf.Tensor,\n",
        "    enc_hidden: tf.Tensor\n",
        ") -> tf.Tensor:\n",
        "    \"\"\"\n",
        "    Perform a single training step.\n",
        "    Args:\n",
        "        source (tf.Tensor): Source language input tensor.\n",
        "        target (tf.Tensor): Target language input tensor.\n",
        "        enc_hidden (tf.Tensor): Initial hidden state for the encoder.\n",
        "        encoder (Encoder): The encoder model.\n",
        "        decoder (Decoder): The decoder model.\n",
        "        target_tokenizer (tf.keras.preprocessing.text.Tokenizer): Tokenizer for the target language.\n",
        "    Returns:\n",
        "        tf.Tensor: The loss value for the training step.\n",
        "    \"\"\"\n",
        "    loss = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_output, enc_hidden = encoder(source, enc_hidden)\n",
        "        dec_hidden = enc_hidden\n",
        "        dec_input = tf.expand_dims(\n",
        "            input=[target_tokenizer.word_index['<start>']] * BATCH_SIZE, # type: ignore\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "        # Teacher forcing\n",
        "        for t in range(1, target.shape[1]): # type: ignore\n",
        "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "            loss += loss_function(target[:, t], predictions) # type: ignore\n",
        "            dec_input = tf.expand_dims(target[:, t], 1) # type: ignore\n",
        "\n",
        "    batch_loss = loss / int(target.shape[1]) # type: ignore\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables)) # type: ignore\n",
        "\n",
        "    return batch_loss # type: ignore\n",
        "\n",
        "\n",
        "def train():\n",
        "    \"\"\"\n",
        "    Train the Seq2Seq model with attention.\n",
        "    \"\"\"\n",
        "    encoder = Encoder(vocab_size=vocab_src_size, embedding_dim=EMBEDDING_DIM, hidden_units=HIDDEN_UNITS, batch_size=BATCH_SIZE)\n",
        "    decoder = Decoder(vocab_size=vocab_tgt_size, embedding_dim=EMBEDDING_DIM, hidden_units=HIDDEN_UNITS, batch_size=BATCH_SIZE)\n",
        "\n",
        "    checkpoint_prefix = os.path.join(CHECKPOINT_DIR, \"ckpt\")\n",
        "    checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)\n",
        "\n",
        "    STEP_PER_EPOCH = len(source_tensor) // BATCH_SIZE\n",
        "\n",
        "    # Training loop\n",
        "    print(\"Starting training...\")\n",
        "    for epoch in range(EPOCHS):\n",
        "        start = time.time()\n",
        "        enc_hidden = encoder.initialize_hidden_state()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch, (source, target) in enumerate(train_dataset.take(STEP_PER_EPOCH)):\n",
        "            batch_loss = train_step(source, target, enc_hidden)\n",
        "            total_loss += batch_loss # type: ignore\n",
        "\n",
        "        if batch % 100 == 0: # type: ignore\n",
        "            print(f'Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy():.4f}') # type: ignore\n",
        "\n",
        "        # Save checkpoint every 1 epochs\n",
        "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
        "\n",
        "        print(f'Epoch {epoch+1} Loss {total_loss / STEP_PER_EPOCH:.4f}') # type: ignore\n",
        "        print(f'Time taken for 1 epoch {time.time() - start:.2f} sec\\n')\n",
        "\n",
        "    print(\"Training complete.\")\n",
        "\n",
        "# Start training the model\n",
        "train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0639cdff",
      "metadata": {
        "id": "0639cdff"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}